---
title: "Machine Learning Project"
author: "Michael A. Kelly"
date: "June 3, 2015"
output: html_document
---

##Abstract:
We analyze data from Groupware related to Human Activity Recognitions. The data contains information on how well 6 different subjects performed barbell lifts where the data was recorded by accelerometers worn by the subjects. They were asked to perform the exercises correctly and then incorrectly in 5 different ways. Our goal is to predict which incorrent or correct method was used based on the provided data. After examining several different prediction methods, we settled on the Random Forest classification model using repeated K-fold cross validation. Our achieved predition accuracy was 99.4%.

####Load all libraries
```{r, InitAnalysis, echo=FALSE}
library(caret); library(kernlab)
library(doParallel);cl <- makeCluster(detectCores()); registerDoParallel(cl)
rm()
setwd("C:/Users/kellym/OneDrive - Brambles/Cousera/8 - Machine Learning/Project Folder")
startTime <- Sys.time()
paste0("Start time: ", startTime)
```

#### Read in the Training Data Set
The data are in two seperate files.The first datafile pml-training.csv contains
19,622 records for the 6 subjects. We will divide this dataset into a training
set and a test set to support cross validation. The second datafile, contains
a set of 20 records to calculate the resulting class for submission as part of
this project.
```{r, LoadData, echo=TRUE}

trainingData <- read.table(file="pml-training.csv", sep=",",header=TRUE, na.strings=c("NA", "#DIV/0!"))
dim(trainingData)
```

#### Clean up the Data Set 

1. Remove rows for new_window = no. All rest of records are NA
2. Remove Columns which have NA values in them. There are many columns with all or mostly all rows contaning NA values. These will not contriute anything to our prediction so we removed them.
3. Remove the first 7 columns. Do not need time, user, and new_window/num_window rows here.
4. Convert all Columns except the last one to data type number.
```{r, CleanData, echo=TRUE}
trainingData <- trainingData[trainingData$new_window=="no",]
trainingData <- trainingData[, apply(trainingData, 2, function(x) !any(is.na(x)))]
trainingData <- trainingData[, -c(1:7)] 
for(i in c(2:ncol(trainingData)-1)) {trainingData[,i] = as.numeric(as.character(trainingData[,i]))}
dim(trainingData)
```

####Create the Training Data Set
In order to perform cross validation, we split the dataset into a training set and a smaller test set. The training set consists of 70% of the data (13,454 records with 52 predictors) which was used for model selection. The testset was used to get an estimate of the out of sample error. 
```{r, CreateTrainingData, cache=TRUE, echo=TRUE}
inTrain <- createDataPartition(y=trainingData$classe, p=0.7, list=FALSE)
training <- trainingData[inTrain,]
testing <- trainingData[-inTrain,]
dim(training)
dim(testing)

```
##Analysis
For this analysis we use the caret package.Random forests improve predictive accuracy by generating a large number of bootstrapped trees  using different random samples of the variables, classifying a case using each tree in this new "forest", and then deciding a final predicted outcome by combining the results across all of the trees using a vote for classification data.

###Method: Using Random Forest ("rf")
```{r, SettrainingControl, echo=TRUE}
sub_i <- sample(1:nrow(training), 100, replace=FALSE, prob=NULL)
set.seed(123); startModTime <- Sys.time() ;paste0("Begin Fit time: ", startModTime)
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 10
)
modFit <- train(classe ~ .,method="rf"
                ,data=training
#                ,subset=sub_i
                ,proxy=TRUE
                ,trControl = fitControl
                ,verbose = FALSE)

modFit
```

####Now get the In Sample Error to use to make the model selections.
Here is show the In Sample error for the final chosen model.
```{r, InSampleError, echo=TRUE}
new_pred <- predict(modFit, training)
confusionMatrix(new_pred, training$classe)

plot(modFit, main="Accuracy Vs. Predictors")
```
Plots have been showing the accuracy of the accuracy of the repeated cross-validation vs the number of randomly selected predictors. We see that after about 28 of these the accuracy drops very rapidly. 

####Now calculate the Out of Sample Error.
We apply our model to the test dataset that we had set asside.
```{r, OutOfSampleError, echo=TRUE}
new_pred <- predict(modFit, testing)
confusionMatrix(new_pred, testing$classe)
```

Now we apply our model to the test dataset and then show the importance of tha variables.
```{r, testSetCases, echo=TRUE}
testingData <- read.table(file="pml-testing.csv"
                           ,sep=","
                           ,header=TRUE
                           ,na.strings=c("NA", "#DIV/0!"))
testset_pred <- predict(modFit, testingData)
testset_pred


print(modFit$finalModel)

EndTime <- Sys.time(); paste0("End time: ", EndTime) ;TotalTime <- EndTime - startTime
paste0("Total Run time: ", TotalTime)

varImp(modFit)
```
###Results
The fit used a repeated K-fold cross validation with 10 folds and 10 repeats.
The in Sample error for this set was 100%. For the out of sample error the 
the model fit only misclassified 50 of the 5763 records yielding an out of sample rate of 99.13% with a 95% Confidence interval from 98.9% to 99.4%.

###Conclusion
The model that was created does a good job with this dataset. Some of
variations of the model also gave very good results. There are other models
and tuning parameters that could be tested in the future, but any better
prediction from them would only be marginal at best.
